name: Greyhounds â€” Daily scrape

on:
  workflow_dispatch:
  schedule:
    # run daily at 00:05 UTC (morning AEST)
    - cron: "5 0 * * *"

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Ensure output folders
        run: |
          mkdir -p data/rns data/thedogs data/combined reports/latest

      # Build reports from whatever valid track PDFs exist in ./forms
      - name: Build from PDFs
        run: |
          python src/run_daily.py

      - name: Show previews
        if: always()
        run: |
          echo "Latest reports:"
          ls -lah reports/latest || true
          echo "Parsed files:"
          ls -lah data/rns || true

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: greyhounds-data
          path: |
            reports/latest/**
            data/rns/**
          if-no-files-found: warn

      - name: Commit results
        run: |
          git config --local user.email "actions-user@github.com"
          git config --local user.name "actions-user"
          git add reports/latest data/rns
          if git diff --staged --quiet; then
            echo "No file changes to commit."
          else
            git commit -m "Auto: update reports and parsed CSVs"
            git push
          fi
